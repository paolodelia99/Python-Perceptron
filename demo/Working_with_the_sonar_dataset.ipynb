{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Try to classify the sonr dataset using the Perceptron\n",
    "\n",
    "Since train the perceptron to learn boolean function is too easy, I tried to \n",
    "use the preceptron to classify some more diffucult: the SONAR Dataset!\n",
    "\n",
    "For those you don't know the SONAR data set which contains the data about 208 patterns \n",
    "obtained by bouncing sonar signals off a metal cylinder (naval mine) and a rock at various \n",
    "angles and under various conditions. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n0         0.0200       0.0371       0.0428       0.0207       0.0954   \n1         0.0453       0.0523       0.0843       0.0689       0.1183   \n2         0.0262       0.0582       0.1099       0.1083       0.0974   \n3         0.0100       0.0171       0.0623       0.0205       0.0205   \n4         0.0762       0.0666       0.0481       0.0394       0.0590   \n..           ...          ...          ...          ...          ...   \n203       0.0187       0.0346       0.0168       0.0177       0.0393   \n204       0.0323       0.0101       0.0298       0.0564       0.0760   \n205       0.0522       0.0437       0.0180       0.0292       0.0351   \n206       0.0303       0.0353       0.0490       0.0608       0.0167   \n207       0.0260       0.0363       0.0136       0.0272       0.0214   \n\n     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n..           ...          ...          ...          ...           ...  ...   \n203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n\n     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n0          0.0027        0.0065        0.0159        0.0072        0.0167   \n1          0.0084        0.0089        0.0048        0.0094        0.0191   \n2          0.0232        0.0166        0.0095        0.0180        0.0244   \n3          0.0121        0.0036        0.0150        0.0085        0.0073   \n4          0.0031        0.0054        0.0105        0.0110        0.0015   \n..            ...           ...           ...           ...           ...   \n203        0.0116        0.0098        0.0199        0.0033        0.0101   \n204        0.0061        0.0093        0.0135        0.0063        0.0063   \n205        0.0160        0.0029        0.0051        0.0062        0.0089   \n206        0.0086        0.0046        0.0126        0.0036        0.0035   \n207        0.0146        0.0129        0.0047        0.0039        0.0061   \n\n     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n0          0.0180        0.0084        0.0090        0.0032   Rock  \n1          0.0140        0.0049        0.0052        0.0044   Rock  \n2          0.0316        0.0164        0.0095        0.0078   Rock  \n3          0.0050        0.0044        0.0040        0.0117   Rock  \n4          0.0072        0.0048        0.0107        0.0094   Rock  \n..            ...           ...           ...           ...    ...  \n203        0.0065        0.0115        0.0193        0.0157   Mine  \n204        0.0034        0.0032        0.0062        0.0067   Mine  \n205        0.0140        0.0138        0.0077        0.0031   Mine  \n206        0.0034        0.0079        0.0036        0.0048   Mine  \n207        0.0040        0.0036        0.0061        0.0115   Mine  \n\n[208 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_1</th>\n      <th>attribute_2</th>\n      <th>attribute_3</th>\n      <th>attribute_4</th>\n      <th>attribute_5</th>\n      <th>attribute_6</th>\n      <th>attribute_7</th>\n      <th>attribute_8</th>\n      <th>attribute_9</th>\n      <th>attribute_10</th>\n      <th>...</th>\n      <th>attribute_52</th>\n      <th>attribute_53</th>\n      <th>attribute_54</th>\n      <th>attribute_55</th>\n      <th>attribute_56</th>\n      <th>attribute_57</th>\n      <th>attribute_58</th>\n      <th>attribute_59</th>\n      <th>attribute_60</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.0323</td>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>...</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>Mine</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows × 61 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 39
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.perceptron import Perceptron\n",
    "from src.activationFunctions.heaviside import Heaviside\n",
    "from src.activationFunctions.relu import ReLU\n",
    "\n",
    "# Train a perceptron to \n",
    "sonar_dataset = pd.read_csv('./data/sonar_csv.csv',delimiter= ',')\n",
    "\n",
    "sonar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# modify the dataset changing the rock with 0 and the mine with 1 \n",
    "# so I can use the heaviside function\n",
    "\n",
    "dfc = sonar_dataset.copy()\n",
    "\n",
    "dfc['Class']=dfc['Class'].apply(lambda t: 0 if t == 'Rock' else 1)\n",
    "\n",
    "dfc.to_csv('./data/mod_sonar.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The perceptron that I've use has 60 weights one bias, and as a activation function I've choose the\n",
    "Heaviside function.\n",
    "\n",
    "The way I trained the perceptron is by using mini batch, cause train the perceptron using only a \n",
    "bigger batch isn't optimal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Perceptron's weights before training:\n",
      "  [0.5054550115195526, 0.22181556659008195, 0.21907273963810514, 0.247649160560664, 0.21140025843647736, 0.7501693202310888, 0.4670194560933446, 0.729844309173593, 0.6720318978922716, 0.8997170554326311, 0.3249023058723459, 0.5151405152881705, 0.27373963214872343, 0.97571018153176, 0.3431856655054466, 0.13515486839871427, 0.2516709980369345, 0.4241665681769947, 0.30213426943492894, 0.1046833314989073, 0.22803803675067125, 0.12870506647034963, 0.33727316381095596, 0.35331852482029913, 0.16372461625273493, 0.6184227088738866, 0.48561624766833256, 0.9896067592042854, 0.660577504638551, 0.5913876173977314, 0.1841404577579594, 0.1829714831692586, 0.6298267753054192, 0.26961715328215863, 0.4175707159696377, 0.08394282535457998, 0.6685496049919417, 0.2857460738408978, 0.4657199815686158, 0.01456943310088965, 0.622652801046051, 0.3212320561564993, 0.9060108907917993, 0.8863221632809157, 0.2848467282646404, 0.000792268120100359, 0.042491844024560965, 0.9394275268527471, 0.4841229338034255, 0.9425223472847255, 0.6911828740316778, 0.4761053428082167, 0.5572179667518782, 0.3814321281926206, 0.579029013328027, 0.6430403272582177, 0.565778162953924, 0.8826320687683998, 0.3792502683662994, 0.9352877568405519]\n",
      "Perceptron's weights after training:\n",
      "  [0.48650601151955275, 0.19554456659008226, 0.18617573963810505, 0.2061111605606641, 0.1679472584364773, 0.63161532023109, 0.34965845609334434, 0.5965453091735928, 0.5668988978922718, 0.746841055432632, 0.17962530587234643, 0.3281305152881704, 0.03780363214872351, 0.7073321815317579, 0.02393266550544687, -0.20414413160128597, -0.08307900196306552, 0.12098356817699475, 0.026505269434928917, -0.20858366850109236, -0.13346596324932902, -0.21267893352965012, -0.04672183618904407, -0.12893747517970092, -0.4003293837472658, -0.02102329112611351, -0.12031075233166771, 0.33581975920428503, 0.01662450463855134, 0.03490661739773124, -0.3715155422420409, -0.3501295168307413, 0.09273377530541911, -0.2907748467178414, -0.10886428403036266, -0.40655417464541993, 0.2678836049919409, 0.04445707384089784, 0.22510798156861606, -0.29951456689910977, 0.3063968010460502, 0.03641505615649926, 0.6755318907917979, 0.7178861632809151, 0.1814577282646399, -0.0751477318798995, -0.034162155975438944, 0.8680375268527463, 0.4476349338034252, 0.9253333472847263, 0.6816948740316796, 0.46716234280821733, 0.5455509667518775, 0.37372712819261994, 0.5713570133280268, 0.631629327258216, 0.5564541629539264, 0.8763860687683983, 0.37070426836629905, 0.9293577568405518]\n",
      "\n",
      "Accuracy:  0.7619047619047619\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filename = './data/mod_sonar.csv'\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "X = df[df.columns[0:61]].values\n",
    "\n",
    "# create the training data set and the test data set\n",
    "X = shuffle(X, random_state=1)\n",
    "train_x, test_x = train_test_split(X, test_size=0.10, random_state=42)\n",
    "\n",
    "# The perceptron has 60 inputs, hence 60 weights, activation function the Heaviside function\n",
    "p = Perceptron(60, Heaviside(), 0.01)\n",
    "\n",
    "# Perceptron weights before training\n",
    "print('Perceptron\\'s weights before training:\\n ', p.weights)\n",
    "\n",
    "p.train(train_x, 5, 1000)\n",
    "\n",
    "# Perceptron weights after training\n",
    "print('Perceptron\\'s weights after training:\\n ', p.weights)\n",
    "\n",
    "no_right = 0\n",
    "\n",
    "for i, r in enumerate(test_x):\n",
    "    p_sol = p.evaluate(r[0:p.no_input])\n",
    "    true_res = r[-1]\n",
    "    no_right += 1 if p_sol == true_res else 0\n",
    "    \n",
    "print('\\nAccuracy: ',no_right/(len(test_x)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}