{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Try to classify the sonr dataset using the Perceptron\n",
    "\n",
    "Since train the perceptron to learn boolean function is too easy, I tried to \n",
    "use the preceptron to classify some more diffucult: the SONAR Dataset!\n",
    "\n",
    "For those you don't know the SONAR data set which contains the data about 208 patterns \n",
    "obtained by bouncing sonar signals off a metal cylinder (naval mine) and a rock at various \n",
    "angles and under various conditions. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n0         0.0200       0.0371       0.0428       0.0207       0.0954   \n1         0.0453       0.0523       0.0843       0.0689       0.1183   \n2         0.0262       0.0582       0.1099       0.1083       0.0974   \n3         0.0100       0.0171       0.0623       0.0205       0.0205   \n4         0.0762       0.0666       0.0481       0.0394       0.0590   \n..           ...          ...          ...          ...          ...   \n203       0.0187       0.0346       0.0168       0.0177       0.0393   \n204       0.0323       0.0101       0.0298       0.0564       0.0760   \n205       0.0522       0.0437       0.0180       0.0292       0.0351   \n206       0.0303       0.0353       0.0490       0.0608       0.0167   \n207       0.0260       0.0363       0.0136       0.0272       0.0214   \n\n     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n..           ...          ...          ...          ...           ...  ...   \n203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n\n     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n0          0.0027        0.0065        0.0159        0.0072        0.0167   \n1          0.0084        0.0089        0.0048        0.0094        0.0191   \n2          0.0232        0.0166        0.0095        0.0180        0.0244   \n3          0.0121        0.0036        0.0150        0.0085        0.0073   \n4          0.0031        0.0054        0.0105        0.0110        0.0015   \n..            ...           ...           ...           ...           ...   \n203        0.0116        0.0098        0.0199        0.0033        0.0101   \n204        0.0061        0.0093        0.0135        0.0063        0.0063   \n205        0.0160        0.0029        0.0051        0.0062        0.0089   \n206        0.0086        0.0046        0.0126        0.0036        0.0035   \n207        0.0146        0.0129        0.0047        0.0039        0.0061   \n\n     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n0          0.0180        0.0084        0.0090        0.0032   Rock  \n1          0.0140        0.0049        0.0052        0.0044   Rock  \n2          0.0316        0.0164        0.0095        0.0078   Rock  \n3          0.0050        0.0044        0.0040        0.0117   Rock  \n4          0.0072        0.0048        0.0107        0.0094   Rock  \n..            ...           ...           ...           ...    ...  \n203        0.0065        0.0115        0.0193        0.0157   Mine  \n204        0.0034        0.0032        0.0062        0.0067   Mine  \n205        0.0140        0.0138        0.0077        0.0031   Mine  \n206        0.0034        0.0079        0.0036        0.0048   Mine  \n207        0.0040        0.0036        0.0061        0.0115   Mine  \n\n[208 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_1</th>\n      <th>attribute_2</th>\n      <th>attribute_3</th>\n      <th>attribute_4</th>\n      <th>attribute_5</th>\n      <th>attribute_6</th>\n      <th>attribute_7</th>\n      <th>attribute_8</th>\n      <th>attribute_9</th>\n      <th>attribute_10</th>\n      <th>...</th>\n      <th>attribute_52</th>\n      <th>attribute_53</th>\n      <th>attribute_54</th>\n      <th>attribute_55</th>\n      <th>attribute_56</th>\n      <th>attribute_57</th>\n      <th>attribute_58</th>\n      <th>attribute_59</th>\n      <th>attribute_60</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.0323</td>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>...</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>Mine</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows Ã— 61 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.perceptron import Perceptron\n",
    "from src.activationFunctions.heaviside import Heaviside\n",
    "\n",
    "# Train a perceptron to \n",
    "sonar_dataset = pd.read_csv('./data/sonar_csv.csv',delimiter= ',')\n",
    "\n",
    "sonar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# modify the dataset changing the rock with 0 and the mine with 1 so I can use the heaviside function\n",
    "\n",
    "dfc = sonar_dataset.copy()\n",
    "\n",
    "dfc['Class']=dfc['Class'].apply(lambda t: 0 if t == 'Rock' else 1)\n",
    "\n",
    "dfc.to_csv('./data/mod_sonar.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The perceptron that I've use has 60 weights one bias, and as a activation function I've choose the\n",
    "Heaviside function.\n",
    "\n",
    "The way I trained the perceptron is by using mini batch, cause train the perceptron using only a \n",
    "bigger batch isn't optimal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.9530954294169789, 0.5570816710423493, 0.9048124443446419, 0.24697727539291636, 0.020185806921819816, 0.5648525262169182, 0.013461548088196551, 0.7023372591614038, 0.32905748342211316, 0.4760524346304603, 0.24723644177784232, 0.4158736556939405, 0.4791583002395532, 0.8467880202422321, 0.38104611660666576, 0.20109970743943517, 0.18771105649386266, 0.9171375632166581, 0.714334993332812, 0.9709167811680939, 0.3273977960124882, 0.28132249612229365, 0.1409252608751207, 0.9932187192971211, 0.7404257970046022, 0.39264468558322363, 0.24614028784304742, 0.5306612538612281, 0.7369151917200663, 0.08929363419515224, 0.14992398180941446, 0.5532184360418618, 0.11424078484892874, 0.45441424557200494, 0.6817984970574861, 0.6741798883027141, 0.44386877622334886, 0.13174929775291788, 0.9014237561285294, 0.791516226832892, 0.691938117109379, 0.05586292963259076, 0.6150119016367103, 0.43569810177452084, 0.6797653719208469, 0.4677780067333762, 0.3466762036411799, 0.021686637911376816, 0.5067124210535786, 0.508261674344989, 0.7183772841575005, 0.6459464343277057, 0.39903832649412596, 0.38329191188404765, 0.37959573696151316, 0.592330912221193, 0.3352957997193433, 0.6527216011820105, 0.4583433329369343, 0.5692800888475649]\n",
      "[0.9189394294169785, 0.5114776710423462, 0.8348604443446425, 0.20306327539291583, -0.05507119307818028, 0.42433152621691816, -0.1609454519118033, 0.44232925916140436, -0.001466516577886602, 0.1290854346304615, -0.10522155822215776, -0.004774344306059492, 0.09948030023955302, 0.463010020242232, -0.0012758833933342387, -0.2830842925605649, -0.32648694350613716, 0.39370956321665906, 0.17086499333281177, 0.3894297811680928, -0.1557662039875118, -0.16779250387770625, -0.3144167391248793, 0.5523797192971197, 0.319311797004603, 0.017403685583223725, -0.27102071215695256, -0.08755774613877207, 0.20303119172006634, -0.3529933658048475, -0.3586040181905855, 0.07444843604186163, -0.245029215151071, 0.10160724557200417, 0.2540614970574856, 0.17830388830271338, -0.14038422377665116, -0.43005970224708195, 0.3566087561285286, 0.3341772268328907, 0.37683611710938014, -0.2266920703674094, 0.38649090163670957, 0.17818310177452126, 0.4517093719208464, 0.33334500673337664, 0.21088720364117972, -0.07245336208862323, 0.470038421053578, 0.4904846743449892, 0.7023362841574986, 0.6377124343277056, 0.39221432649412674, 0.37336991188404717, 0.370158736961513, 0.5785219122211913, 0.321034799719344, 0.6465556011820087, 0.45207333293693475, 0.5633130888475609]\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  0\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  0\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  0.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "Perceptron evluation:  1\n",
      "Actual result:  1.0\n",
      "0.5458937198067633\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# The perceptron has 60 inputs, hence 60 weights, as a \n",
    "p = Perceptron(60, Heaviside(), 0.01)\n",
    "\n",
    "# Perceptron weights before training\n",
    "print(p.weights)\n",
    "\n",
    "reader = pd.read_csv('./data/mod_sonar.csv', sep=',')\n",
    "\n",
    "mini_batch = []\n",
    "\n",
    "# Train the perceptron using mini_batch\n",
    "for i, r in reader.iterrows():\n",
    "    row = r.array\n",
    "    if i != 0 and i%5 == 0:\n",
    "        p.train(mini_batch, 100)\n",
    "        mini_batch = [row]\n",
    "    else:\n",
    "        mini_batch.append(row)\n",
    "\n",
    "# Perceptron weights after training\n",
    "print(p.weights)\n",
    "\n",
    "no_right = 0\n",
    "\n",
    "for i, r in reader.iterrows():\n",
    "    p_sol = p.evaluate(r[0:p.no_input])\n",
    "    true_res = r[-1]\n",
    "    # print('Perceptron evluation: ',p_sol)\n",
    "    # print('Actual result: ', true_res)\n",
    "    no_right += 1 if p_sol == true_res else 0\n",
    "    \n",
    "print(no_right/207)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}