{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Try to classify the sonr dataset using the Perceptron\n",
    "\n",
    "Since train the perceptron to learn boolean function is too easy, I tried to \n",
    "use the preceptron to classify some more diffucult: the SONAR Dataset!\n",
    "\n",
    "For those you don't know the SONAR data set which contains the data about 208 patterns \n",
    "obtained by bouncing sonar signals off a metal cylinder (naval mine) and a rock at various \n",
    "angles and under various conditions. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n0         0.0200       0.0371       0.0428       0.0207       0.0954   \n1         0.0453       0.0523       0.0843       0.0689       0.1183   \n2         0.0262       0.0582       0.1099       0.1083       0.0974   \n3         0.0100       0.0171       0.0623       0.0205       0.0205   \n4         0.0762       0.0666       0.0481       0.0394       0.0590   \n..           ...          ...          ...          ...          ...   \n203       0.0187       0.0346       0.0168       0.0177       0.0393   \n204       0.0323       0.0101       0.0298       0.0564       0.0760   \n205       0.0522       0.0437       0.0180       0.0292       0.0351   \n206       0.0303       0.0353       0.0490       0.0608       0.0167   \n207       0.0260       0.0363       0.0136       0.0272       0.0214   \n\n     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n..           ...          ...          ...          ...           ...  ...   \n203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n\n     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n0          0.0027        0.0065        0.0159        0.0072        0.0167   \n1          0.0084        0.0089        0.0048        0.0094        0.0191   \n2          0.0232        0.0166        0.0095        0.0180        0.0244   \n3          0.0121        0.0036        0.0150        0.0085        0.0073   \n4          0.0031        0.0054        0.0105        0.0110        0.0015   \n..            ...           ...           ...           ...           ...   \n203        0.0116        0.0098        0.0199        0.0033        0.0101   \n204        0.0061        0.0093        0.0135        0.0063        0.0063   \n205        0.0160        0.0029        0.0051        0.0062        0.0089   \n206        0.0086        0.0046        0.0126        0.0036        0.0035   \n207        0.0146        0.0129        0.0047        0.0039        0.0061   \n\n     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n0          0.0180        0.0084        0.0090        0.0032   Rock  \n1          0.0140        0.0049        0.0052        0.0044   Rock  \n2          0.0316        0.0164        0.0095        0.0078   Rock  \n3          0.0050        0.0044        0.0040        0.0117   Rock  \n4          0.0072        0.0048        0.0107        0.0094   Rock  \n..            ...           ...           ...           ...    ...  \n203        0.0065        0.0115        0.0193        0.0157   Mine  \n204        0.0034        0.0032        0.0062        0.0067   Mine  \n205        0.0140        0.0138        0.0077        0.0031   Mine  \n206        0.0034        0.0079        0.0036        0.0048   Mine  \n207        0.0040        0.0036        0.0061        0.0115   Mine  \n\n[208 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_1</th>\n      <th>attribute_2</th>\n      <th>attribute_3</th>\n      <th>attribute_4</th>\n      <th>attribute_5</th>\n      <th>attribute_6</th>\n      <th>attribute_7</th>\n      <th>attribute_8</th>\n      <th>attribute_9</th>\n      <th>attribute_10</th>\n      <th>...</th>\n      <th>attribute_52</th>\n      <th>attribute_53</th>\n      <th>attribute_54</th>\n      <th>attribute_55</th>\n      <th>attribute_56</th>\n      <th>attribute_57</th>\n      <th>attribute_58</th>\n      <th>attribute_59</th>\n      <th>attribute_60</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>Rock</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.0323</td>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>...</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>Mine</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>Mine</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows Ã— 61 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.perceptron import Perceptron\n",
    "from src.activationFunctions.heaviside import Heaviside\n",
    "from src.activationFunctions.relu import ReLU\n",
    "\n",
    "# Train a perceptron to \n",
    "sonar_dataset = pd.read_csv('./data/sonar_csv.csv',delimiter= ',')\n",
    "\n",
    "sonar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# modify the dataset changing the rock with 0 and the mine with 1 so I can use the heaviside function\n",
    "\n",
    "dfc = sonar_dataset.copy()\n",
    "\n",
    "dfc['Class']=dfc['Class'].apply(lambda t: 0 if t == 'Rock' else 1)\n",
    "\n",
    "dfc.to_csv('./data/mod_sonar.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The perceptron that I've use has 60 weights one bias, and as a activation function I've choose the\n",
    "Heaviside function.\n",
    "\n",
    "The way I trained the perceptron is by using mini batch, cause train the perceptron using only a \n",
    "bigger batch isn't optimal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.08997591854520393, 0.1500385455515243, 0.4593635673375629, 0.3215008448332125, 0.7144679531579338, 0.7969917243211957, 0.6326959316816979, 0.15884730290911508, 0.5580948816639809, 0.8478833184893434, 0.38311887030924485, 0.46405073952582654, 0.7480195188700858, 0.7322899521649519, 0.2747507509254654, 0.4343105370390191, 0.9539284093489537, 0.11258932796179322, 0.4748261749311543, 0.8458347314946022, 0.45784325536490766, 0.7643963887944148, 0.34330554379912814, 0.44953258228768156, 0.8189413673508164, 0.7612591590372509, 0.5686979928218102, 0.5273624685894885, 0.40255312713584335, 0.47630236288725825, 0.12507294074246644, 0.9439166332009262, 0.34826610488108267, 0.8808228027199558, 0.5307567864550439, 0.09198834549995194, 0.04326816217580809, 0.7012519938351593, 0.4370564585657598, 0.42926512042238574, 0.8619955995949605, 0.34599390849819256, 0.7877798015192201, 0.4285908541517288, 0.8327438440201556, 0.8129599141774336, 0.7409838524978933, 0.3972248810582589, 0.46378076693416703, 0.8962680356376711, 0.23499954030060333, 0.9366086325703694, 0.58965953549245, 0.5416324361269648, 0.07106333599232584, 0.10608832343233043, 0.49720443353345023, 0.3205213168903014, 0.6644260799452889, 0.6649558889547768]\n",
      "[0.040937918545203955, 0.10523154555152403, 0.39393156733756307, 0.2756288448332131, 0.5483529531579368, 0.6014597243211895, 0.4894639316817002, 0.0009963029091144984, 0.48548988166398127, 0.714282318489346, 0.18756087030924493, 0.16149973952582725, 0.36648951887008874, 0.35938495216495125, -0.24123524907453311, -0.25185346296098216, 0.24584240934895252, -0.6003926720382078, 0.0008161749311546818, 0.49942973149460107, 0.009723255364907792, 0.2162803887944151, -0.11426545620087253, -0.06384941771231895, 0.09560436735081557, 0.010565159037248837, -0.08809600717818927, 0.13023746858948826, 0.006048127135843677, 0.037272362887259484, -0.5259650592575357, 0.3990566332009199, -0.3241268951189179, 0.11391380271995705, -0.11373121354495679, -0.435373654500049, -0.3451048378241917, 0.5318519938351589, 0.10041945856576166, -0.3638688795776149, 0.05461059959495736, -0.32430909150180764, 0.2770288015192218, 0.0911658541517306, 0.5733978440201565, 0.5077869141774369, 0.4849288524978908, 0.271295881058257, 0.44471376693416315, 0.8687490356376675, 0.22050354030060174, 0.9189916325703718, 0.5709785354924528, 0.5388544361269656, 0.06287233599232499, 0.0906123234323301, 0.4811304335334516, 0.31480031689030336, 0.6545750799452901, 0.6568158889547777]\n",
      "0.3285024154589372\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filename = './data/mod_sonar.csv'\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "X = df[df.columns[0:61]].values\n",
    "\n",
    "X = shuffle(X, random_state=1)\n",
    "train_x, test_x = train_test_split(X, test_size=0.40, random_state=42)\n",
    "\n",
    "# The perceptron has 60 inputs, hence 60 weights, as a \n",
    "p = Perceptron(60, Heaviside(), 0.01)\n",
    "\n",
    "# Perceptron weights before training\n",
    "print(p.weights)\n",
    "\n",
    "mini_batch = []\n",
    "\n",
    "# Train the perceptron using mini_batch\n",
    "for i, r in enumerate(train_x):\n",
    "    row = r\n",
    "    if i != 0 and i%5 == 0:\n",
    "        p.train(mini_batch, 50)\n",
    "        mini_batch = [row]\n",
    "    else:\n",
    "        mini_batch.append(row)\n",
    "\n",
    "# Perceptron weights after training\n",
    "print(p.weights)\n",
    "\n",
    "no_right = 0\n",
    "\n",
    "for i, r in enumerate(test_x):\n",
    "    p_sol = p.evaluate(r[0:p.no_input])\n",
    "    true_res = r[-1]\n",
    "    # print('Perceptron evluation: ',p_sol)\n",
    "    # print('Actual result: ', true_res)\n",
    "    no_right += 1 if p_sol == true_res else 0\n",
    "    \n",
    "print(no_right/207)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}