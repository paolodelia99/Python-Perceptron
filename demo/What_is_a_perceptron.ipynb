{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## What is a Percepton\n",
    "\n",
    "A percepton is a type of binary classifier. A binary classfier is a function\n",
    "which can be decide weather or not an input, represented by a vector of numbers,\n",
    "belong to a specific class. \n",
    "\n",
    "The perceptron maps its input $x$ ( a real value vector ) to an output value $f(x)$ ( a single binary value ):\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\chi(\\langle w, x \\rangle + b)\n",
    "\\end{equation}\n",
    "\n",
    "where $w$ is the vector of the weights with real value, $\\langle \\cdot , \\cdot \\rangle$ is the scalar product,\n",
    "$b$ is the bias,  a constant term that doesn't dipend on any input value and $\\chi(x)$ is the output fuction, also called \n",
    "activation function. \n",
    "The most common choices for $\\chi(x)$ are:\n",
    "\n",
    "- $\\chi(x)$ = $sing(x)$\n",
    "- $\\chi(x)$ = $Θ(x)$\n",
    "- $\\chi(x)$ = $x$\n",
    "\n",
    "where $Θ(x)$ is the Heavside Function.\n",
    "\n",
    "![perceptron](../assets/img/perceptron.png)\n",
    "\n",
    "\n",
    "The perceptron works weel when the learning set is linearly separable, while when the learning isn't linearly separable\n",
    "its learning algorithm doesn't terminate. If the vector are not linearly separable will never reach a point where all vectors are \n",
    "classified properly. The most famous example of perceptron inability to solve problems with linearly separable vector is the \n",
    "boolean **XOR** problem.\n",
    "\n",
    "![lin_space](../assets/img/linearly_and_non_linearly_space.png)\n",
    "\n",
    "## How to train the Perceptron\n",
    "\n",
    "There are various way to train a perceptron, one of the most effienct ( the one that used here )\n",
    "is the **Delta Rule**\n",
    "\n",
    "The Delta Rule is a gradient descend learning rule for updating the weights of the inputs of an artifical \n",
    "neuron in a single-layer neural network. It is a special case of the more general backpropagation algorithm. For\n",
    "a neuron $j$ with activation function $g(x)$, the delta rule for $j$'s weight $w_{ji}$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{ji} = \\eta (t_j - y_j)g'(h_i)x_i\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\eta$ is a small constant, called learning rate\n",
    "- $g(x)$ is the neuron activation function\n",
    "- $g'$ is the derivative of $g$\n",
    "- $t_j$ us the target output\n",
    "- $h_i$ is the weighted sum of the neuron's inputs\n",
    "- $y_j$ is the actual output\n",
    "- $x_i$ is the ith input\n",
    "\n",
    "It holds that $h_j = \\sum x_i w_{ji}$ and $ y_j = g(h_j)$. \n",
    "The delta rule is commonly stated is a simplified forn for a neuron with \n",
    "a linear activation function as \n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{ji} = \\eta (t_j - y_j)x_i\n",
    "\\end{equation}\n",
    "\n",
    "While the delta rule is similar to the percetron's training rule, the derivation is different.\n",
    "If the percepton uses the Heaviside step function as the activation function , it turn out that \n",
    "$g(h)$ is no differantiable in zero, which make the direct application of the delta rule impossible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}